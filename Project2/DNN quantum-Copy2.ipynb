{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hn = 3\n",
    "class Model(nn.Module):\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stepLength = 1.2\n",
    "        self.hidden = torch.zeros((1, hn))\n",
    "        \n",
    "        self.Whh = nn.Linear(hn, hn)\n",
    "        self.Whx = nn.Linear(1, hn, bias = False)\n",
    "        \n",
    "        self.layer1 = nn.Linear(1 + hn, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 8)\n",
    "        self.layer4 = nn.Linear(8, 1)\n",
    "      \n",
    "    \n",
    "    def forward_RNN(self, x):\n",
    "        self.hidden = torch.tanh(self.Whh(self.hidden) + self.Whx(x))\n",
    "        \n",
    "        \n",
    "    def forward_DNN(self, x):\n",
    "        \n",
    "        y = torch.cat((self.hidden, x), 1)\n",
    "        \n",
    "        y = torch.tanh(self.layer1(y))\n",
    "        y = torch.tanh(self.layer2(y))\n",
    "        y = torch.tanh(self.layer3(y))\n",
    "        y = self.layer4(y)\n",
    "     \n",
    "        return y*torch.exp(-0.1*x**2)\n",
    "    \n",
    "    def sample(self, N):\n",
    "        total = 0\n",
    "        x = torch.Tensor(4*np.random.random((N,1)) - 2)\n",
    "        psi_old = self.forward_DNN(x)\n",
    "        \n",
    "        for i in range(10):\n",
    "            x_new = x + 1.5*torch.Tensor(2*np.random.random((N,1)) - 1)\n",
    "            psi_new = self.forward_DNN(x_new)\n",
    "            \n",
    "            bool_array = (psi_new/psi_old)**2 > torch.Tensor(np.random.random((N,1)))\n",
    "            x[bool_array] = x_new[bool_array]\n",
    "            psi_old[bool_array] = psi_new[bool_array]\n",
    "            total += 1\n",
    "            \n",
    "        return x, total\n",
    "    \n",
    "    def resetHidden(self, N):\n",
    "        self.hidden = torch.zeros((N, hn))\n",
    "        self.forward_RNN(torch.zeros((N, 1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using naive minimization of energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:10<16:52, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, Grad: 580.4075927734375, Energy: 6.806632995605469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:48<14:38,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, Grad: 177.06956481933594, Energy: 5.639733791351318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [02:14<14:58, 10.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e26330a9b4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpsi_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpsi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdfdx\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0md2fdx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mkinetic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2fdx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpsi_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e26330a9b4f3>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpsi_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpsi2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdfdx\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_total\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0md2fdx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mkinetic1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md2fdx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mpsi_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/newhd/Documents/neural/env_neural/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "N = 1000\n",
    "for epoch in tqdm(range(epochs)): \n",
    "    psi_total = 1\n",
    "\n",
    "    model.resetHidden(N) \n",
    "    x1 = model.sample(N)[0].detach().requires_grad_()\n",
    "    psi1 = model.forward_DNN(x1)\n",
    "\n",
    "    model.forward_RNN(x1)\n",
    "    x2 = model.sample(N)[0].detach().requires_grad_()\n",
    "    psi2 = model.forward_DNN(x2)\n",
    "\n",
    "    psi_total = psi1*psi2\n",
    "    dfdx   = [torch.autograd.grad(a, x1, create_graph=True)[0][i] for i, a in enumerate(psi_total)]\n",
    "    d2fdx2 = [torch.autograd.grad(a, x1, create_graph=True)[0][i] for i, a in enumerate(dfdx)]\n",
    "\n",
    "    kinetic1 = -0.5*torch.Tensor(d2fdx2)/psi_total\n",
    "    potential1 = 0.5*x1**2\n",
    "\n",
    "    dfdx   = [torch.autograd.grad(a, x2, create_graph=True)[0][i] for i, a in enumerate(psi_total)]\n",
    "    d2fdx2 = [torch.autograd.grad(a, x2, create_graph=True)[0][i] for i, a in enumerate(dfdx)]\n",
    "\n",
    "    kinetic2 = -0.5*torch.Tensor(d2fdx2)/psi_total\n",
    "    potential2 = 0.5*x2**2\n",
    "\n",
    "    E_L = (kinetic1 + potential1 + kinetic2 + potential2 + 1/torch.abs(x1-x2)).detach()\n",
    "    \n",
    "    PE = torch.mean(psi_total/psi_total.detach()*E_L)\n",
    "    P  = torch.mean(psi_total/psi_total.detach())\n",
    "    E  = torch.mean(E_L)   \n",
    "\n",
    "    \n",
    "    loss = 2*(PE - P*E)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    grad = 0\n",
    "    for param in model.parameters():\n",
    "            grad += torch.sum(param.grad**2)\n",
    "\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"epoch: {epoch}, Grad: {grad}, Energy: {E.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x_lin = torch.linspace(-5, 5, 100).reshape(100,-1)\n",
    "\n",
    "    model.resetHidden(100)\n",
    "    psi1 = model.forward_DNN(x_lin)[:,0].detach().numpy()\n",
    "\n",
    "    x = torch.Tensor([[x]])\n",
    "\n",
    "    model.forward_RNN(x)\n",
    "    psi2 = model.forward_DNN(x_lin)[:,0].detach().numpy()\n",
    "\n",
    "\n",
    "    plt.plot(x_lin[:,0], psi1**2/np.sum(psi1**2))\n",
    "    plt.plot(x_lin[:,0], psi2**2/np.sum(psi2**2))\n",
    "    plt.plot(x, 0.01, \"o\")\n",
    "    plt.xlim((-5,5))\n",
    "    plt.ylim((0, 0.05))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "interact(f, x=(-4.0, 4., 0.1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = torch.linspace(-4, 4, 100).reshape(100,-1)\n",
    "psi = model.forward(x_lin)[:,0].detach().numpy()\n",
    "\n",
    "plt.plot(x_lin[:,0], psi**2)\n",
    "plt.plot(x_lin[:,0], 1/np.sqrt(np.pi)*np.exp(-x_lin[:,0]**2), \"--\")\n",
    "plt.legend([\"DNN\", \"analytical\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "E = 0\n",
    "for i in tqdm(range(N)):\n",
    "    model.resetHidden() \n",
    "    x1 = model.sample()[0].detach().requires_grad_()\n",
    "    psi1 = model.forward_DNN(x1)\n",
    "\n",
    "    model.forward_RNN(x1)\n",
    "    x2 = model.sample()[0].detach().requires_grad_()\n",
    "    psi2 = model.forward_DNN(x2)\n",
    "\n",
    "    psi_total = psi1*psi2\n",
    "    dfdx, = torch.autograd.grad(psi_total, x1, create_graph=True)\n",
    "    d2fdx2, = torch.autograd.grad(dfdx, x1, create_graph=True)\n",
    "\n",
    "    kinetic1 = -0.5*d2fdx2/psi_total\n",
    "    potential1 = 0.5*x1**2\n",
    "\n",
    "    dfdx, = torch.autograd.grad(psi_total, x2, create_graph=True)\n",
    "    d2fdx2, = torch.autograd.grad(dfdx, x2, create_graph=True)\n",
    "\n",
    "    kinetic2 = -0.5*d2fdx2/psi_total\n",
    "    potential2 = 0.5*x2**2\n",
    "\n",
    "    E += (kinetic1 + potential1 + kinetic2 + potential2 + 1/torch.abs(x1-x2)).detach()\n",
    "\n",
    "E = E/N\n",
    "\n",
    "print(E.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added penalization of small amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "model = Model()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "N = 50\n",
    "mu = 0.5\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    PE_acc = 0\n",
    "    P_acc = 0\n",
    "    E_acc = 0\n",
    "    grad = 0\n",
    "    psi_pen = 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        x = model.sample()[0].detach().requires_grad_()\n",
    "     \n",
    "        psi = model.forward(x)\n",
    "        psi_pen += (1 - psi**2)**2\n",
    "        dfdx, = torch.autograd.grad(psi, x, create_graph=True)\n",
    "        d2fdx2, = torch.autograd.grad(dfdx, x, create_graph=True)\n",
    "        \n",
    "        kinetic = -0.5*d2fdx2/psi\n",
    "        potential = 0.5*x**2\n",
    "        \n",
    "        E_L = (kinetic + potential).detach()\n",
    "        PE_acc += psi/psi.detach()*E_L\n",
    "        P_acc  += psi/psi.detach()\n",
    "        E_acc  += E_L   \n",
    "    \n",
    "    PE_acc /= N\n",
    "    P_acc  /= N\n",
    "    E_acc  /= N\n",
    "    psi_pen /= N\n",
    "    \n",
    "    E = 2*(PE_acc - P_acc*E_acc) + mu*psi_pen\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    E.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "            grad += torch.sum(param.grad**2)\n",
    "\n",
    "    \n",
    "    if (epoch%10 == 0):\n",
    "        print(f\"epoch: {epoch}, Grad: {grad}, Energy: {E_acc.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing auto grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  432., 13824.], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-03d675704be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(x.grad, y.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdfdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-03d675704be5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(x.grad, y.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdfdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfdx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/newhd/Documents/neural/env_neural/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    155\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    156\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "x = [torch.Tensor([3]).requires_grad_(), torch.Tensor([6]).requires_grad_()]\n",
    "y = torch.Tensor([4, 8]).requires_grad_()\n",
    "\n",
    "f = torch.Tensor(x)**3*y**2\n",
    "\n",
    "print(f)\n",
    "#f.backward()\n",
    "#print(x.grad, y.grad)\n",
    "\n",
    "dfdx = [torch.autograd.grad(a, b, create_graph=True)[0] for a, b in zip(f, x)]\n",
    "print(dfdx)\n",
    "\n",
    "d2fdx2 = [torch.autograd.grad(a, b, create_graph=True)[0] for a, b in enumerate(dfdx, x)]\n",
    "\n",
    "print(d2fdx2)\n",
    "\n",
    "#d2fdx2.backward()\n",
    "#print(x.grad, y.grad)\n",
    "\n",
    "#print(dfdx)\n",
    "\n",
    "\n",
    "#ddfdxdx, = torch.autograd.grad(dfdx, x, create_graph=True)\n",
    "#print(ddfdxdx.grad_fn(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_neural",
   "language": "python",
   "name": "env_neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
