\section{Method}\label{sec:Method}
\subsection{Architecture}

\tikzset{%
  base/.style = {rectangle, rounded corners, draw=black, minimum width=1cm,
    minimum height=1cm, text centered},
  net/.style = {rectangle, rounded corners, fill=green!50, minimum width=1cm,
    minimum height=1cm, text centered},
  connector/.style={
    -latex,
    font=\scriptsize
  },
  rectangle connector/.style={
    connector,
    to path={(\tikztostart) -- ++(#1,0pt) \tikztonodes |- (\tikztotarget) },
    pos=0.5
  },
  rectangle connector/.default=-2cm,
  straight connector/.style={
    connector,
    to path=--(\tikztotarget) \tikztonodes
  },
  >=latex
}
\begin{figure*}[t!]
  \centering
\begin{tikzpicture}[node distance=2cm, align=center]
  \tikzstyle{var}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
  \tikzstyle{annot} = [text width=4em, text centered]

  % Leftmost cell
 \node (xi0)  [var, pin=left:\(\tilde{x}_{0}^{0}\)] {\(\tilde{x}_{i}^{0}\)};
 \node (dnn0)  [net, right of=xi0] {DNN};
 \node (psi0)  [var, above of=dnn0] {\(\psi_{i}^{0}\)};
 \node (hidden0) [var, below of=dnn0] {\(h_{0}\)};
 \node (rnn0) [net, right of=hidden0] {RNN};
 \node (mcmc0) [annot] at ($(xi0)!0.5!(psi0)$) {MCMC};

 \definecolor{vmc}{rgb}{0.2,0.6,1}
 \draw[->] (xi0) -- (dnn0);
 \draw[->] (dnn0) -- (psi0);
 \draw[->] (psi0) -| (xi0);
 \draw[->] (hidden0) -- (dnn0);
 \draw[->] (hidden0) -- (rnn0);
 \draw[->] ([xshift=0.7cm]mcmc0) arc (0:340:0.7cm);
 \draw[->] (xi0) -- (xi0 |- {{(0,-3)}}) -| (rnn0) node [below, midway, left=1.5cm, yshift=-0.5cm]{\(\tilde{x}_{\text{final}}^{0}\)};
 %\draw[->, out=270] (xi0) -| node[yshift=1.25cm, text width=3cm]{stuff}  (rnn0);

 % Second cell
 \node (xi1)  [var, right of=dnn0, pin=left:\(\tilde{x}_{0}^{1}\)] {\(\tilde{x}_{i}^{1}\)};
 \node (dnn1)  [net, right of=xi1] {DNN};
 \node (psi1)  [var, above of=dnn1] {\(\psi_{i}^{1}\)};
 \node (hidden1) [var, below of=dnn1] {\(h_{1}\)};
 \coordinate[right of=hidden1] (rnn1);
 \node (mcmc1) [annot] at ($(xi1)!0.5!(psi1)$) {MCMC};

 \draw[->] (rnn0) -- (hidden1);
 \draw[->] (xi1) -- (dnn1);
 \draw[->] (dnn1) -- (psi1);
 \draw[->] (psi1) -| (xi1);
 \draw[->] (hidden1) -- (dnn1);
 \draw[dotted] (hidden1) -- (rnn1);
 \draw[->] ([xshift=0.7cm]mcmc1) arc (0:340:0.7cm);
 \draw[-, dotted] (xi1) -- (xi1 |- {{(0,-1)}}) -| ([xshift=-0.25cm]$(dnn1)!0.5!(hidden1)$) arc (180:0:0.25cm)
          -- +(1.7,0);

 % Final cell
 \node (dots) [annot, right of=dnn1] {\(\ldots\)};
 \node (dots_2) [annot, xshift=-1cm] at (rnn1) {\(\ldots\)};
 \node (xi2)  [var, right of=dnn1, xshift=2cm, pin=left:\(\tilde{x}_{0}^{N}\)] {\(\tilde{x}_{i}^{N}\)};
 \node (dnn2)  [net, right of=xi2] {DNN};
 \node (psi2)  [var, above of=dnn2] {\(\psi_{i}^{N}\)};
 \node (hidden2) [var, below of=dnn2] {\(h_{N}\)};
 \coordinate[right of=hidden2] (rnn2);
 \node (mcmc2) [annot] at ($(xi2)!0.5!(psi2)$) {MCMC};

 \draw[dotted] ([xshift=2cm]rnn1) -- (hidden2);
 \draw[->] (xi2) -- (dnn2);
 \draw[->] (dnn2) -- (psi2);
 \draw[->] (psi2) -| (xi2);
 \draw[->] (hidden2) -- (dnn2);
 \draw[->] ([xshift=0.7cm]mcmc2) arc (0:340:0.7cm);
 %\draw[->, out=190, in=290] (hidden2) to (hidden0);

 % PSI
 \node (Psi) [var, yshift=2cm] at ($(psi0)!0.5!(psi2)$) {\(\Psi\)};
 \draw[->] (psi0) -- (Psi) node[midway, above, sloped] {\(\psi_{\text{final}}^{0}\)};
 \draw[->] (psi1) -- (Psi)node[midway, above, sloped] {\(\psi_{\text{final}}^{1}\)};
 \draw[->] (psi2) -- (Psi)node[midway, above, sloped] {\(\psi_{\text{final}}^{N}\)};
 \coordinate[right of=psi1](psii);
 \draw[->, dotted] (psii) -- (Psi)node[midway, above, sloped] {\(\psi_{\text{final}}^{i}\)};

 % Cell Region
 \fill[blue!20, nearly transparent, rounded corners] ($(xi0) + (-1.4,0)$) -- ($(xi0) + (-1.4, -4)$) -- ($(rnn0) + (1,-2)$)
 --($(rnn0) + (1,0.8)$) -- ($(hidden0) + (0.7,0.8)$) -- ($(psi0) + (0.7,1)$) -- ($(xi0) + (-1.4,3)$)-- cycle;
\end{tikzpicture}
\caption{\label{fig:architecture}
  Diagrammatic overview of the neural network architecture for a system of \(N\)
  bosons. The shaded
  blue region is a single ``cell'' consisting of a RNN to update the hidden
  state, a DNN to evaluate the position wave function, and a brute force
  Metropolis sampler to sample the position space. The Metropolis sampling yields a
  probable position for the particle, \(\tilde{x}_{\text{final}}^{i}\), which is
  taken as the ``actual'' position of the particle and used in the RNN to update the hidden state. Each particle in the system
  corresponds to one cell, with the subsequent particles depending on the former
  through the hidden state. Once all of the particles have been iterated through, the
  combined wave function \(\Psi\) can be obtained from their single
  particle wave functions. Each MCMC sampling is initialized by random positions
\(\tilde{x}_{0}^{i}\)}
\end{figure*}

A novel neural network architecture has been built to obtain uncorrelated
samples from the position wave function of bosons in finite potentials. A
diagram is shown in~\cref{fig:architecture}.
It consists of three main parts: a DNN, a RNN and a MCMC sampler.


The DNN receives the position \(\vb{x}_i\) of a particle concatenated with the hidden state from the RNN as input and evaluates the
conditional wave function \(\psi(\vb{x}_i|\vb{x}_{i-1}, \cdots, \vb{x}_{0})\). It has two fully connected layers of
\(32\) or \(64\) neurons. Both \(\tanh\) and \(ReLu\) were tried as activation
functions to study their influence on the result. 


The position space is explored by the MCMC sampler to
obtain the most probable positions. A brute force Metropolis sampler is used for
this purpose. The sampling is initialized by a number of walkers with random
position drawn from a uniform distribution \(U(\vb{x}_{\text{min}}, \vb{x}_{\text{max}})\), where the bounds were
set to ensure that most (\(>97\%\)) of the probability distribution is enclosed. After a set number of iterations the sampler returns the last sampled
position, which is used as the \textit{actual} position of the particle in
question.


The RNN takes the last sampled position to update the hidden state. It has a
single layer with five neurons and a \(\tanh\) activation function. 
By encoding the position of the last particle in the hidden state, the wave
function of the subsequent particle becomes dependent on the position of all
previous particles by the autoregressive property of the RNN. The resulting
Metropolis samples are therefore uncorrelated, obviating the need for blocking.

\subsection{Automatic Differentiation in Tensorflow}
Since we are studying Hamiltonians defined on continuous space, calculating the
Laplacian of the trial wave function is necessary to account for the kinetic
energy of the system. Since the RNN-DNN model is made using the popular neural
network framework Tensorflow(2), it feels natural to use its automatic
differentiation functionality to calculate the Laplacian of the model with
respect to the inputs(the positions). However, for more than one dimensional
input, it turns out to be quite difficult, although a work-around proposed by~\cite{laplace}
resolved the difficulty. The problem was as follow:  

Starting with an input $x$ of dimension \textsf{(batch\_size, d)}, a forward
pass through the model results in an output $y$ of dimension $(\textsf{batch\_size}, 1)$,
as the wave function evaluates to a scalar value. When performing
\mintinline{python}{grad_y = tf.gradients(y,x)}, we get  
\begin{equation*}
	\textsf{grad\_y} = \left(\frac{\partial y}{\partial x_1}, \cdots, \frac{\partial y}{\partial x_d}\right),
\end{equation*}
which has dimensional $(\textsf{batch\_size}, d)$. The problem arises when
calculating the second
derivative \mintinline{python}{grad2_y = tf.gradients(grad_y,x)}, symbolically

\begin{equation*}
	\textsf{grad2\_y} = \left(\sum_{i=1}^{d}\frac{\partial^2 y}{\partial x_1 \partial x_i}, \cdots, \sum_{i=1}^{d}\frac{\partial^2 y}{\partial x_1 \partial x_i}\right).
\end{equation*}
Since \mintinline{python}{tf.gradients} is meant to be used for gradient
descent, a summation of the derivatives of all outputs w.r.t the input $x$ is
performed, which should not be present in the Laplacian. 
The na√Øve solution is to slice the output after the first differentiation, such as\newline 
\mintinline{python}{tf.gradients(grad_y[i],x[i])}. Sadly, slicing this way does not respect
the computational graph constructed during forward pass, and \mintinline{python}{x[i]} is
interpreted as a new variable not related to \mintinline{python}{grad_y[i]}. The fix is to
use clever reshaping of the input variable \mintinline{python}{x} before passing it to the
model, in order to allow for the wanted slicing without destroying the
computational graph. Details can be found in~\cite{laplace} and in our source code.  

\subsection{Optimization}
During the training procedure of the RNN-DNN model, we use the following expression for the loss value:
\begin{equation*}
	L = 2\sum_{i=1}^{N_B}\ln(\psi_i) (E_i - E)
\end{equation*}
where $N_B$ is the batch size, $\psi_i$ and $E_i$ are the wave function and
local energy of sample $i$ of the batch, and $E$ is the average local energy
over the whole batch. During gradient decent, the derivative of the loss value
is differentiated w.r.t. the model parameters, and the gradient describes (ref)
is recovered. 

During vanilla gradient decent, the parameters are updated by performing a small
step in the opposite direction of the computed gradient 

\begin{equation*}
	\theta \rightarrow \theta - \mu \nabla_\theta \langle E \rangle,
\end{equation*}
where $\theta$ is the parameters of the model, $\mu$ is the learning rate, and $\nabla_\theta \langle E \rangle$ is the derivative of the loss value. 

As a better alternative to vanilla gradient decent, we use Tensorflows ADAM
optimizer with default values, as it is a popular choice in the machine learning
community.   

\subsection{One-Body Density}
To extract a one-body density from a Monte-Carlo simulation, the space is
partitioned into a number of bins in an appropriate range where the wave
function is large. The bin size can be chosen small to get finer details of the
density, but will require more data to mitigate statistical error. 

For each particle configuration produced at every Metropolis step, the number of
particles coinciding with each bin is checked. The one-body density is then
produced by averaging over all Metropolis steps. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
